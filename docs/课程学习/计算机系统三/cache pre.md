# Cache Pre

可以在处理器需要某一数据之前，由编译器插入请求该数据的预取指令。

寄存器预取 与 缓存预取。

循环是重要的预取优化目标。

发出预取指令会导致指令开销。

基于数组的数值代码获得相当大的成功

基于指针的应用中的潜力。因为基于指针的应用，由于复杂的数据结构和递归引用难度较大。RDS

软件控制的预取技术的一个潜在缺点是需要额外的指令开销来发出预取指令。然而，对于今天的超标量处理器来说，很可能能够将这种开销与其他计算重叠。软件控制的方法具有需要较少硬件支持（硬件已经存在）、更加灵活以及（最重要的）能够利用关于未来访问模式的应用程序特定知识等主要优势。

任何软件控制的预取方案都可以看作有两个主要阶段。首先，分析阶段会预测哪些动态内存引用可能会遭受缓存未命中，因此应该进行预取。其次，调度阶段尝试提前足够长的时间插入预取，以有效地隐藏延迟，同时引入最小的运行时开销。

对于基于数组的应用程序，编译器可以使用局部性分析来预测哪些动态引用需要预取，并使用循环拆分和软件流水线来调度预取 [15]。

数组引用和指针引用之间的一个基本区别是地址的生成方式。对于数组引用 A[i]，一旦选择了 i 的值，就可以计算出其地址。相反，对于指针引用\*p，除非读取 p 中存储的值，否则其地址是未知的。这种差异使得对于 RDS 来说，分析和调度阶段比数组更具挑战性。

然而，尽管最近在针对堆分配对象的指针分析技术方面取得了显著进展 [6、8、10]，编译器仍然没有足够的复杂性自动区分这两种情况。总的来说，分析堆分配对象的地址对于编译器来说是一个非常困难的问题。

我们安排预取请求的能力也受限于节点是通过指针链接在一起的事实。例如，考虑图 2（a）中显示的情况，假设需要三个节点的计算才能隐藏延迟，我们希望在访问节点 ni 时启动对节点 ni+a 的预取。问题在于，为了计算节点 ni+a 的地址，我们必须首先解引用节点 ni+2 中的指针，而要做到这一点，我们必须首先解引用节点 ni+h 中的指针等等。因此，我们不能预取（或获取）未来节点，直到它和当前节点之间的所有节点都已获取。然而，触及这些中间节点的行为意味着我们无法容忍获取超过一个节点的延迟。例如，图 2（b）中的预取代码将无法隐藏任何延迟，而图 2（c）中的代码可能会运行得更快，因为它具有较少的指令开销。这个例子说明了我们所说的指针追踪问题。由于调度 RDS 预取是如此困难的问题，我们使其成为本文的主要焦点。分析的改进倾向于通过消除不必要的预取来减少预取开销。

LEMON：这里用链表状结构更加方便。

然而，使用历史指针预取可以在随后的 RDS 遍历中几乎完全隐藏延迟，而贪心预取只能隐藏延迟的一部分。虽然历史指针预取提供了潜在的优势，但代价是两种额外的开销：(i) 执行开销用于构建历史指针，和 (ii) 存储这些新指针的空间开销。为了最小化执行开销，我们可以根据 RDS 结构的变化速度降低历史指针的更新频率。极端情况下，如果 RDS 没有变化，我们只需要设置历史指针一次。空间开销的问题是，它可能会使缓存行为变得更糟。消除这种空间开销的愿望是我们下一个预取方案的动机。

问题是如何以及何时可以进行数据线性化的映射（即将 RDS 映射到连续的内存位置上）？理论上，可以在 RDS 最初构建之后动态重新映射数据，但这样做可能会导致运行时开销大，也可能会违反程序语义。相反，最好在创建时映射节点，如果创建顺序已经与遍历顺序匹配，或者可以安全地重新排序以匹配，则适用。由于动态重新映射开销大（或不可能），因此如果 RDS 的结构变化缓慢（或根本不变化），则此方案的效果最佳。

## 讲稿（草稿）

我们首先讲讲为什么需要 prefetch，这其实是由处理器和内存之前的性能差异越来越大导致的。

我们首先来看一下没有 prefetch 的情况，假设我们的程序如图所示，是需要连续访问内存中的连续地址。我们看到每个指令在执行前都需要进行长时间的 fetch 阶段。这样会使得执行阶段有大量的 idle。

```go
for i := 0; i<100 ; i++ {
	a[i] = b[i]
}
```

那么如果我们执行 prefetch 的话，那么 idle 就会大大的降低。我门重叠内存访问与计算。这些操作是相互独立的，不会互相影响。这种技术可以减少内存访问的等待时间，从而提高程序的执行效率。

做这种 prefetch 分为硬件 prefetch 和软件 prefetch 两种。那么 software prefetching 的优势体现在哪里呢。一个是，我们可以知道整个程序的运行状态，因此我们可以合理地插入预取数据指令。

接下来我们首先分析 Prefetching for Arrays。假设我们有下面的代码。

```c
for i = 0 to 2
	for j = 0 to 99
		A[i][j] = B[j + 1][0] * B[j][0]
```

这里 A 是一个 3 行 100 列的数组，而 B 是一个 100 行 3 列的数组。

这里的红色代表 miss，白色代表 hit。

我们对 hit 和 miss 的情况进行分析，假设一个 cache line 可以容纳两个元素。，我们这里可以看到，A 这个元素可以受益于空间局部性。当我们访问了 A【i】【j】的时候，那么我们接下来访问的 A【i】【j+1】必定是会 hit 的。我们再来看 B，我们的 B 是不受益于空间局部性的，但是 B 受益于时间局部性。在这里表现为我们的 B【j】【0】只会 miss 一次。我们可以发现我们总共有 251 次 miss，这显然是不能接受的。

然后我们假设我们需要 7 个周期才能 fetch 到对应的数据，也即我们需要提前 7 个进行 prefetch。我们的编译器在编译时插入了插入了一些 prefetch 指令，而且我们注意到这里将这个 loop 进行了拆分。这时候我们再分析 miss，我们看到对 a，b 而言，都只有前 7 个可能会产生 miss，而在后面的数据则是由于 prefetch 不再 miss 了。因此我们这里只有 19 个 miss。

```c
for (j  =  0; j < 100; j = j + 1) { 
    prefetch(b[j + 7][0]); 
    /* b(j,0) for 7 iterations later */ 
    prefetch(a[0][j + 7]); 
    /* a(0,j) for 7 iterations later */ 
    a[0][j]  = b[j][0] * b[j + 1][0] ; }
 for (i  =  1; i < 3; i  =  i + 1) 
    for (j  =  0; j < 100; j  =  j + 1) { 
	    prefetch(a[i][j + 7]); 
	    /* a(i,j) for + 7 iterations */ 
	    a[i][j]  =  b[j][0] * b[j + 1][0];}

```

接下来看一些测试程序在 prefetch for arrays 上的表现。这里分别是一些不同类型的测试程序，综合型，核心型，数学库，应用型，并行型等等都有，然后蓝色的部分代表 instruction，红色的部分代表 memory access stall。

接下来讲完了 array prefetching，我们接下来讲一讲 prefetching for Recursive Data Structures。那么首先什么是 recursive data structure 呢？我们熟知的 link list, tree 等情况都是 recursive data structure。

这样的 recursive data structure 有一个共同的特点，就是他需要一步一步获得目标。

就比方说，我们的 fetch 需要三个周期去完成，那么我们在处理 $n_i$ 节点的时候，我们可能想要去获得 $n_{i+3}$ 这个节点的数据。于是想当然的话，我们可能会这么写

```c
while (p) { 
	prefeteh(p->next); 
	work(p->data); 
	p = p->next; 
} 
```

但是我们来看，当我们想要获得 A_i+3 的时候，我们肯定会去 fetch A_i+1 和 A_i+2，这样的话我们这样的执行其实和这样的和单独 prefetch 下一条指令是相差不大的。

当然，即便是只 prefetch 了后一条指令，那么我们的效率仍然是得到了提高了的。假设我们的 work() 耗时为 Wi，而我们的 Load 耗时为 Li。那么在不加 prefetch 的情况下，如图，我们每一条指令，先会进行 load，再进行 work。这样的话效率是 1/（L+W）。

但是我们加了 prefetch 下一条指令的时候，我们可以看到，在我们 work 的时候，我们同步执行了 load，这样的话我们的效率大约为 1/L。

但是我们肯定不满足于仅仅如此，我们肯定想要的是更加的效率，如果我们能和之前数组一样的，可以稳定预取到后面的结果，我们肯定是可以获得更好的效果。

第一种考虑的策略是 gredy alorithm。他适用于这样一种情况，就是一个节点他可能含有很多指针，我们假设有 k 个指针。我们知道在特定的程序中只有其中一个指针可以立即跟随控制流作为遍历中的下一个节点。但是剩余的 k - 1 个指针作为自然跳跃指针，可以在第一次访问节点时立即预取。我们在之后的访问中，仍然有可能这第一步 fetch 的节点，如果这个将来的点被访问到，那么这个访问延迟会被隐藏部分。

我们再以二叉树的前序遍历来说明贪心的 prefetch。我们在遇到一个节点的时候，我们会 prefetch 他的左节点和右节点。我们假设 process() 会消耗一个单位的时间，而 prefetch 会消耗两个单位的时间。我们可以看到对于一半的节点，他的 latency 降到了一半。对于另一半的节点，他会 cache hit。

第二种方法是 History-Pointer Prefetching。这个方法试用在我们遍历这个递归数据结构的方式在一段时间内不会发生改变的时候。

问题是如何以及何时可以进行数据线性化的映射（即将 递归数据结构 映射到连续的内存位置上）？理论上，可以在 RDS 最初构建之后动态重新映射数据，但这样做可能会导致运行时开销大，也可能会违反程序语义。

限制侵略性，以防过于侵略地追求会导致有用数据被驱逐出缓存，预取数据提前过多，甚至是纯粹浪费的无用预取。这是一个平衡的过程，旨在在与访问模式和工作负载需求相符的微架构资源使用效率的同时最大化收益。