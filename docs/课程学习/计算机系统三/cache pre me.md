# Cache Pre Me

接下来讲完了 array prefetching，我们接下来讲一讲 prefetching for Recursive Data Structures。那么首先什么是 recursive data structure 呢？我们熟知的 link list, tree 等情况都是 recursive data structure。这样的 recursive data structure 有一个共同的特点，就是他需要一步一步获得目标。

就比方说，我们的 fetch 需要三个周期去完成，那么我们在处理 $n_i$ 节点的时候，我们可能想要去获得 $n_{i+3}$ 这个节点的数据。于是想当然的话，我们可能会这么写

但是我们来看，当我们想要获得 A_i+3 的时候，我们肯定会去 fetch A_i+1 和 A_i+2，这样的话我们这样的执行其实和这样的和单独 prefetch 下一条指令是相差不大的。

当然，即便是只 prefetch 了后一条指令，那么我们的效率仍然是得到了提高了的。

假设我们的 work() 耗时为 Wi，而我们的 Load 耗时为 Li。那么在不加 prefetch 的情况下，如图，我们每一条指令，先会进行 load，再进行 work。这样的话效率是 1/（L+W）。

但是我们加了 prefetch 下一条指令的时候，我们可以看到，在我们 work 的时候，我们同步执行了 load，这样的话我们的效率大约为 1/L。

但是我们肯定不满足于仅仅如此，我们肯定想要的是更加的效率，如果我们能和之前数组一样的，可以稳定预取到后面的结果，我们肯定是可以获得更好的效果。

第一种考虑的策略是 gredy alorithm。他适用于这样一种情况，就是一个节点他可能含有很多指针，我们假设有 k 个指针。我们知道在特定的程序中只有其中一个指针可以立即跟随控制流作为遍历中的下一个节点。但是剩余的 k - 1 个指针作为自然跳跃指针，可以在第一次访问节点时立即预取。我们在之后的访问中，仍然有可能这第一步 fetch 的节点，如果这个将来的点被访问到，那么这个访问延迟会被隐藏部分。

我们再以二叉树的前序遍历来说明贪心的 prefetch。我们在遇到一个节点的时候，我们会 prefetch 他的左节点和右节点。我们假设 process() 会消耗一个单位的时间，而 prefetch 会消耗两个单位的时间。我们可以看到对于一半的节点，他的 latency 降到了一半。对于另一半的节点，他会 cache hit。

第二种方法是 History-Pointer Prefetching。这个方法试用在我们遍历这个递归数据结构的方式在一段时间内不会发生改变的时候。

问题是如何以及何时可以进行数据线性化的映射（即将 递归数据结构 映射到连续的内存位置上）？理论上，可以在 RDS 最初构建之后动态重新映射数据，但这样做可能会导致运行时开销大，也可能会违反程序语义。

限制侵略性，以防过于侵略地追求会导致有用数据被驱逐出缓存，预取数据提前过多，甚至是纯粹浪费的无用预取。这是一个平衡的过程，旨在在与访问模式和工作负载需求相符的微架构资源使用效率的同时最大化收益。
