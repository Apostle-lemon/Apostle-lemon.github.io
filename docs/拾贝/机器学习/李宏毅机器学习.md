# 李宏毅机器学习

## 机器学习基础概念

机器学习 ≈ looking for function

- different types of function  

regression 回归：输出是 scalar  
classification：输出是类别  
structured learning : 输出 image, document, etc.

- How to find a function
1. function with unknown parameters 也即我们需要写出带有未知参数的函式。这个函式需要你的 domain knowledge。你需要猜测这个函式长什么样。这个 Function 就是 model 模型。已经知道的东西 feature 特征。
2. define loss from training data。需要根据我们的 parameter 得到损失值这样一个损失函数。这是从训练资料进行计算的。MAE mean abosolute error 绝对值误差平均。单次的误差为 $e=\left |y - \hat{y}\right |$ , 总的平均误差即为 $L = \frac{1}{N}\sum_{n}e_{n}$。当然我们也可以采用 MSE mean square error 的方式，此时的 $e=\left ( y - \hat{y}\right )^{2}$。还有 cross-entropy 等方式。
3. Optimization。修改参数的值，使得损失 $L$ 最小。这时候我们可以将原来的模型中的参数（我们不妨用 $w$ 表示）表示为 $w^{*}$。表示其为最优。可以采用 gradient discent 梯度下降等算法。<p></p>  
在梯度下降中，最初始的的参数的值为 $w^{0}$，随机初始化。我们需要计算当前点的斜率 $\frac{\partial L}{\partial w}|_{w=w^{0}}$。调整的步伐大小 = $\eta$ × 斜率。这里的 $\eta$ 是 learning rate，是一个 hyperparameters 超参数。然后不断地反复进行更新，最终 $w$ 不再变化的值就是最优的值。

gradient discent 可能会找到 local minima 而不是 global minima。如果有两个参数，也是按照同样的方式调整。类似于 wx + b 这样的模型叫做 linear model。

![](img/23b926dff4573d69665ff980f551309a_MD5.png)

linear model 只能是线性关系。这种来自 model 的限制叫做 model bias。我们可以复杂化模型，设置为 constant + 一些别的函数。

piecewise linear curve 折线。即便是曲线，当我们的点取得足够多的时候，我们所得到的折线图也会和原曲线非常类似。我们可以用 sigmoid function S 型曲线来逼近这么个蓝色曲线。$y = c\space signoid(b+wx_1)$。上边的蓝色 function 被理解为 hard sigmoid。最终我们可以得到 $y = b+\sum_{i} c_i\space sigmoid(b_i+w_ix_1)$。

![](img/6993acbba59af8c8eabff102d34eef8f_MD5.png)

如果是考虑多个 x 造成的影响，那么我们得到的参数就是 $y=b+\sum_ic_i\space sigmoid(b_i+\sum_jw_{i,j}x_{j})$。我们可以表示成向量的模样。

![](img/8598f9214cf3a10c1ac736ef92d8e74d_MD5.png)

我们可以用线性代数的方式来表示这些向量，并且可以通过将 column（或者是 row 也可以）拿出来，将所有的参数拼成一个一列的参数 $\theta$。

在用 sigmoid 函数的情况下，定 Loss 并没有不同，optimization 也没有不同。有一些符号上的方便书写的方式。Loss 损失函数 $L(\theta)$，gradient 表示为 $g=\nabla L(\theta^{0})$。因为计算 Loss 变得复杂，因此我们可能只拿出其中一个 batch 算 lose。一个 epoch 就是将所有的 batch 都经历一遍。
![](img/cb8bc0eaaee2f1a09d08a024c8291218_MD5.png)

我们可以用 ReLU Rectified Linear Units 线性修正单元激活函数来代替 sigmoid。sigmoid function，ReLU 都被称为 activation function。

这些 sigmoid 或者 ReLU 就叫做 Neuron，如果进行了多次的反复，那么就是 Neural Network。按照李宏毅的说法，Neural Network 已经被玩坏了，因此给了这些 Neuron 以新的名字。每一层都叫做 hidden layer。如果有很多层 layer，那么就叫做 Deep，所以是 Deep learning。

## Colab 教程

Colab Demo [https://reurl.cc/ra63jE](https://reurl.cc/ra63jE)

在 colab 中，使用！会开启一个新的 shell 执行命令，然后关闭这个 shell。使用 % 则会视作为 magic function，会进行全局的修改。

使用 google 提供的 CPU 的方式是 runtime 执行阶段> change runtime type 变更执行类型 > hardware accelerator 硬件加速器 > GPU。这样的选择会 restart section。

点击程式码左边的开始执行即可运行程式。左侧文件夹可以看到下载的内容。注意这些内容只是暂时存储在这里，想要永久保存需要下载到自己的电脑上。colab 可以连接到 goole 云端硬盘。这样的话我们的下载就可以永久保存，而不需要手动再次下载到自己的电脑上。

下载 google colab 的方式是 file > download .ipynb。或者可以 file > save a copy in Drive。

## Pytorch 教程

- pytorch 环境配置
